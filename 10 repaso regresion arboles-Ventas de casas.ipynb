{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89a07af",
   "metadata": {},
   "source": [
    "* [1. Importando librerias](#section1)\n",
    " * [1.1 Visualizacion de todas las filas y columnas](#section11)\n",
    "* [2 Arboles de Regresion con Sklearn](#section2)\n",
    "* [3 Metricas de Modelo Regresion Arboles- Sklearn](#section3)\n",
    " * [3.1 MSE](#section31)\n",
    "   * [3.1.1 Calculo](#section311)\n",
    "   * [3.1.2 Codigo](#section312)\n",
    " * [3.2 RMSE](#section32)\n",
    "   * [3.2.1 Calculo](#section321)\n",
    "   * [3.2.2 Codigo](#section322)\n",
    " * [3.3 MAE](#section33)\n",
    "   * [3.3.1 Calculo](#section331)\n",
    "   * [3.3.2 Calculo](#section332)\n",
    " * [3.4 Mape(Mean Absolute Percentage Error)](#section34)\n",
    " * [3.5 R2 (coeficiente de determinacion)](#section35)\n",
    "   * [3.5.1 Calculo](#section351)\n",
    "   * [3.5.2 Codigo](#section352)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a81c5",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#004D7F\">1. Importando Librerias</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2652808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9528c71",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a>\n",
    "## <font color=\"#004D7F\">1.1 Visualizacion de todas las filas y columnas </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f8d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba587d35",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#004D7F\">2 Arboles de Regresion con Sklearn</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4469415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Datos/housing_data.txt',header=None, sep=\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca18c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2182e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7188feba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "\n",
       "   PTRATIO      B  LSTAT  MEDV  \n",
       "0     15.3  396.9   4.98  24.0  \n",
       "1     17.8  396.9   9.14  21.6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41653cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff94a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decision tree regressor.\n",
      "\n",
      "    Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    criterion : {\"mse\", \"friedman_mse\", \"mae\", \"poisson\"}, default=\"mse\"\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"mse\" for the mean squared error, which is equal to variance\n",
      "        reduction as feature selection criterion and minimizes the L2 loss\n",
      "        using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
      "        squared error with Friedman's improvement score for potential splits,\n",
      "        \"mae\" for the mean absolute error, which minimizes the L1 loss using\n",
      "        the median of each terminal node, and \"poisson\" which uses reduction in\n",
      "        Poisson deviance to find splits.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "            Poisson deviance criterion.\n",
      "\n",
      "    splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "        The strategy used to choose the split at each node. Supported\n",
      "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "        the best random split.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the randomness of the estimator. The features are always\n",
      "        randomly permuted at each split, even if ``splitter`` is set to\n",
      "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "        select ``max_features`` at random at each split before finding the best\n",
      "        split among them. But the best found split may vary across different\n",
      "        runs, even if ``max_features=n_features``. That is the case, if the\n",
      "        improvement of the criterion is identical for several splits and one\n",
      "        split has to be selected at random. To obtain a deterministic behaviour\n",
      "        during fitting, ``random_state`` has to be fixed to an integer.\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    min_impurity_split : float, default=0\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19. The default value of\n",
      "           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "           will be removed in 1.0 (renaming of 0.25).\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the\n",
      "        (normalized) total reduction of the criterion brought\n",
      "        by that feature. It is also known as the Gini importance [4]_.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    max_features_ : int\n",
      "        The inferred value of max_features.\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    tree_ : Tree instance\n",
      "        The underlying Tree object. Please refer to\n",
      "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "        for basic usage of these attributes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DecisionTreeClassifier : A decision tree classifier.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "           Learning\", Springer, 2009.\n",
      "\n",
      "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_diabetes\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> from sklearn.tree import DecisionTreeRegressor\n",
      "    >>> X, y = load_diabetes(return_X_y=True)\n",
      "    >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      "    >>> cross_val_score(regressor, X, y, cv=10)\n",
      "    ...                    # doctest: +SKIP\n",
      "    ...\n",
      "    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      "           0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(DecisionTreeRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447304a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### variables utiizadas en el algoritmo\n",
    "x = df.drop(\"MEDV\", axis=1).values\n",
    "y = df['MEDV'].values\n",
    "\n",
    "### aplicando el algoritmo - entrenando una regresión lineal con una variable\n",
    "arbolito= DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)\n",
    "arbolito.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1f1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicho_arbolito_reg=arbolito.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d8ef8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicho_arbolito_reg=pd.DataFrame(y_predicho_arbolito_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b24fda",
   "metadata": {},
   "source": [
    "#### Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "328551a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.7875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.7875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0  23.7875\n",
       "1  23.7875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicho_arbolito_reg.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f4c6b",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#004D7F\">3 Metricas de Modelo Regresion Arboles- Sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37a17f",
   "metadata": {},
   "source": [
    "## Métricas Regression Model\n",
    "\n",
    "La mayoría de los principiantes y profesionales no se preocupan por el rendimiento del modelo. Se habla de construir un modelo bien generalizado pero el modelo de aprendizaje automático no puede tener una eficiencia del 100% sino mas bien el modelo tiene un nivel de error que además incluye el concepto de sobreajuste y subajuste.\n",
    "\n",
    "Es necesario obtener la precisión en los datos train, pero también es importante obtener un resultado genuino y aproximado en los datos test, de lo contrario el modelo no sirve de nada.\n",
    "\n",
    "Por lo tanto, para construir e implementar un modelo generalizado necesitamos evaluar el modelo en diferentes métricas que nos ayuden a optimizar mejor el rendimiento, afinarlo y obtener un mejor resultado. Por lo tanto aqui vamos a entender de que trata cada métrica que vamos a mostrar para entender los beneficios y las desventajas de evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea529c",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "## <font color=\"#004D7F\">3.1 MSE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbe5a6",
   "metadata": {},
   "source": [
    "- No es robusto a valores **ATIPICOS**\n",
    "- Variable de entrada **\"metros\"**\n",
    "- Variable de salida **\"metros.metros\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c7810",
   "metadata": {},
   "source": [
    "### MSE (Mean Squared Error)\n",
    "\n",
    "El MSE es una métrica muy utilizada y muy simple este indica que la diferencia cuadrática entre el valor real y el valor estimado.\n",
    "\n",
    "¿Qué representa realmente el MSE? Representa la distancia al cuadrado entre los valores reales y los valores estimados, realizamos el cuadrado para evitar la cancelación de los términos negativos.\n",
    "\n",
    "$$MSE= \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "**Ventajas del MSE**\n",
    "\n",
    "- La gráfica del MSE es diferenciable, por lo que se puede utilizar fácilmente como función de pérdida.\n",
    "\n",
    "**Desventajas del MSE**\n",
    "\n",
    "- El valor que se obtiene después de calcular el MSE es una unidad de salida al cuadrado. Por ejemplo, la variable de salida está en metros (m), entonces después de calcular el MSE la salida que obtenemos está en metros al cuadrado.\n",
    "- Si hay valores atípicos en el conjunto de datos, se penalizan más los valores atípicos y el MSE calculado es mayor. Así que, en resumen, no es robusto a los valores atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667e6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y)  ### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e302822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.7875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.7875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0  23.7875\n",
       "1  23.7875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicho_arbolito_reg.head(2) ###pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc0ef7",
   "metadata": {},
   "source": [
    "<a id=\"section311\"></a>\n",
    "## <font color=\"#004D7F\">3.1.1 Calculo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a2fdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13.063983\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((y-y_predicho_arbolito_reg)**2).sum())/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded37c82",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "## <font color=\"#004D7F\">3.1.2 Codigo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5379f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.063983256643901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y,y_predicho_arbolito_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2df61",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "## <font color=\"#004D7F\">3.2 RMSE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619e1c3",
   "metadata": {},
   "source": [
    "- No es robusto a valores **ATIPICOS**\n",
    "- Variable de entrada **\"metros\"**\n",
    "- Variable de salida **\"metros\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afea9d",
   "metadata": {},
   "source": [
    "### RMSE (Root Mean Squared Error)\n",
    "\n",
    "La raiz del error cuadrático medio (RMSE) es la raíz cuadrada de MSE. Se usa más comúnmente que MSE porque, en primer lugar, a veces el valor de MSE puede ser demasiado grande para compararlo fácilmente.\n",
    "\n",
    "$$MSE= \\sqrt(\\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2)$$\n",
    "\n",
    "**Ventajas del RMSE**\n",
    "- El valor de salida que se obtiene está en la misma unidad que la variable de salida requerida, lo que facilita la interpretación de la pérdida.\n",
    "\n",
    "**Desventajas del RMSE**\n",
    "- No es tan robusto a los valores atípicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2771c",
   "metadata": {},
   "source": [
    "<a id=\"section321\"></a>\n",
    "## <font color=\"#004D7F\">3.2.1 Calculo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff105d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.614413\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((((y-y_predicho_arbolito_reg)**2).sum())/df.shape[0])**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82f789",
   "metadata": {},
   "source": [
    "<a id=\"section322\"></a>\n",
    "## <font color=\"#004D7F\">3.2.2 Codigo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d22ac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6144132658903163"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y,y_predicho_arbolito_reg)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27802c",
   "metadata": {},
   "source": [
    "<a id=\"section33\"></a>\n",
    "## <font color=\"#004D7F\">3.3 MAE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434068d2",
   "metadata": {},
   "source": [
    "- Es mas robusto a valores **ATIPICOS** que **RMSE Y MSE**\n",
    "- Variable de entrada **\"metros\"**\n",
    "- Variable de salida **\"metros\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811a75f",
   "metadata": {},
   "source": [
    "### MAE (Mean Absolute Error)\n",
    "\n",
    "El error absoluto medio (MAE) es similar al error cuadrático medio (MSE). Sin embargo, en lugar de la suma del cuadrado del error en MSE, MAE está tomando la suma del valor absoluto del valor real vs el valor estimado.\n",
    "\n",
    "$$MAE= \\frac{1}{n} \\sum_{i=1}^n \\vert{y^{(i)} - \\hat{y}^{(i)}}\\vert$$\n",
    "\n",
    "**Ventajas del MAE**\n",
    "\n",
    "- El MAE que se obtiene está en la misma unidad que la variable de salida.\n",
    "- Es más robusto a los valores atípicos.\n",
    "\n",
    "**Desventajas de MAE**\n",
    "\n",
    "- La gráfica del MAE no es diferenciable por lo que tenemos que aplicar varios optimizadores como el Descenso Gradiente que sí puede ser diferenciable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72891e9",
   "metadata": {},
   "source": [
    "<a id=\"section331\"></a>\n",
    "## <font color=\"#004D7F\">3.3.1 Calculo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4644ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.486419\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(y-y_predicho_arbolito_reg).sum())/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424e440",
   "metadata": {},
   "source": [
    "<a id=\"section332\"></a>\n",
    "## <font color=\"#004D7F\">3.3.2 Calculo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14f74500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4864194197243825"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y,y_predicho_arbolito_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3498b",
   "metadata": {},
   "source": [
    "<a id=\"section34\"></a>\n",
    "## <font color=\"#004D7F\">3.4 Mape(Mean Absolute Percentage Error)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6b68b",
   "metadata": {},
   "source": [
    "### MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "MAPE da el error en términos de porcentaje es decir que porcentaje del valor real esta sesgado el valor estimado.\n",
    "\n",
    "Mientras que el MAE te da un valor que puedes discutir si es aceptable o no la métrica nunca te da una pista de cuánto \"error\" es aceptable.\n",
    "Por ejemplo, si tiene un error absoluto medio de 10 dólares, ¿es demasiado? ¿O es aceptable? esto depende de la escala de su valor real. Por ejemplo si tenemos un valor predicho de 100 y el valor real es de 150, el error porcentual absoluto es (150 - 100) / 150 que da 33%.\n",
    "\n",
    "$$MAPE= \\frac{1}{n} \\sum_{i=1}^n \\frac{{y^{(i)} - \\hat{y}^{(i)}}}{y^{(i)}}$$\n",
    "\n",
    "**Algunas características**\n",
    "\n",
    "- Puede tener algunos problemas para tratar los errores cuando el valor real es 0. Cuando hay algunos ejemplos como éste en su muestra, se recomiendan otras métricas.\n",
    "- El MAPE tiene algunos problemas con los valores cercanos a cero. Si se esperan estos valores en su objetivo, debería elegir otra métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f6e0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y,y_predicho_arbolito_reg):\n",
    "    return ((abs(y-y_predicho_arbolito_reg)/y).sum())/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee76f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12.57168\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y,y_predicho_arbolito_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903995d0",
   "metadata": {},
   "source": [
    "<a id=\"section35\"></a>\n",
    "## <font color=\"#004D7F\">3.5 R2 (coeficiente de determinacion)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497cdbf",
   "metadata": {},
   "source": [
    "### R2 score\n",
    "\n",
    "R2 score es una métrica que indica el rendimiento de su modelo, no el sesgo que tienen las predicciones. Así, con la ayuda de R2 score tenemos un modelo de referencia para comparar vs un modelo minimo cosa que ninguna de las otras métricas proporciona. Entonces, básicamente, R2 score calcula en qué medida la línea de regresión es mejor que una línea media.\n",
    "\n",
    "R2 score también se conoce como Coeficiente de Determinación o a veces también se conoce como Bondad de ajuste.\n",
    "$$R^2= 1 - \\frac{SSE}{SST}$$\n",
    "Donde: \n",
    "\n",
    "**SSE** error de la suma cuadrada de la línea de regresión.\n",
    "\n",
    "**SST** error de la suma cuadrada de la línea media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe25d90",
   "metadata": {},
   "source": [
    "<a id=\"section351\"></a>\n",
    "## <font color=\"#004D7F\">3.5.1 Calculo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee52306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6610.375528\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse=((y-y_predicho_arbolito_reg)**2).sum();sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bac00651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    42716.295415\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst=((y-y.mean())**2).sum();sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2793a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.845249\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-sse/sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c86f7",
   "metadata": {},
   "source": [
    "<a id=\"section352\"></a>\n",
    "## <font color=\"#004D7F\">3.5.2 Codigo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad8a3d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452493254942353"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y,y_predicho_arbolito_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
